{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training - using CONDA Dataset\n",
    "\n",
    "**Roshan Saigal & Mickey Paulus**\n",
    "\n",
    "# CONDA Model Training Pipeline\n",
    "\n",
    "This Python Notebook trains the following three models: Multinomial Naive Bayes, Logistic Regression, and Latent Dirichlet Allocation (LDA), on the preprocessed CONDA dataset.\n",
    "\n",
    "## Summary\n",
    "- Loads preprocessed features and labels from the preprocessing pipeline\n",
    "- Splits data into training and testing sets (80/20 split)\n",
    "- Trains three models:\n",
    "  - Multinomial Naive Bayes\n",
    "  - Logistic Regression\n",
    "  - Latent Dirichlet Allocation (LDA) with LDA-based classifier\n",
    "- Saves trained models for later inference\n",
    "\n",
    "**Note:** Model evaluation, comparison, and visualization are performed in `model_analysis.ipynb`.\n",
    "\n",
    "## Inputs\n",
    "- `preprocessing_outputs/features/X_tfidf.npz` – TF-IDF matrix\n",
    "- `preprocessing_outputs/features/y.npy` – Binary labels\n",
    "- `preprocessing_outputs/conda_cleaned_binary_without_chat.csv` – Cleaned text data for LDA\n",
    "\n",
    "## Outputs\n",
    "- `model_outputs/naive_bayes_model.joblib` – trained Multinomial Naive Bayes\n",
    "- `model_outputs/logistic_regression_model.joblib` – trained Logistic Regression\n",
    "- `model_outputs/lda_model.joblib` – trained LDA model\n",
    "- `model_outputs/lda_count_vectorizer.joblib` – Count vectorizer for LDA\n",
    "- `model_outputs/lda_lr_classifier.joblib` – LDA-based classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Set Paths for Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing input paths\n",
    "PREPROCESSING_DIR = \"../without_chat_time/preprocessing_outputs\"\n",
    "FEATURES_DIR      = f\"{PREPROCESSING_DIR}/features\"\n",
    "\n",
    "# Model output paths\n",
    "OUTPUT_DIR    = \"../without_chat_time/model_outputs\"\n",
    "MODELS_DIR    = OUTPUT_DIR  \n",
    "\n",
    "import os\n",
    "for d in [OUTPUT_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from joblib import dump\n",
    "\n",
    "# Scikit-learn library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Setting a seed for reproducibility\n",
    "RANDOM_STATE = hash(\"I love MAXXXXXXX!\") % (2**32)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TF-IDF features shape: (26914, 6171)\n",
      "Sparsity: 99.96%\n",
      "Loaded labels shape: (26914,)\n",
      "\n",
      "Class distribution:\n",
      "  Non-Toxic (0): 21694 samples (80.6%)\n",
      "  Toxic (1): 5220 samples (19.4%)\n"
     ]
    }
   ],
   "source": [
    "# Load TF-IDF features (sparse matrix)\n",
    "X = sparse.load_npz(f\"{FEATURES_DIR}/X_tfidf.npz\")\n",
    "print(f\"Loaded TF-IDF features shape: {X.shape}\")\n",
    "print(f\"Sparsity: {1.0 - X.nnz / (X.shape[0] * X.shape[1]):.2%}\")\n",
    "\n",
    "# Load labels\n",
    "y = np.load(f\"{FEATURES_DIR}/y.npy\")\n",
    "print(f\"Loaded labels shape: {y.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = \"Toxic\" if label == 1 else \"Non-Toxic\"\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  {class_name} ({label}): {count} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 21531 samples\n",
      "Testing set size: 5383 samples\n",
      "  Non-Toxic (0): 17355 (80.6%)\n",
      "  Toxic (1): 4176 (19.4%)\n",
      "  Non-Toxic (0): 4339 (80.6%)\n",
      "  Toxic (1): 1044 (19.4%)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 80% training and 20% testing, this ratio is standard for training models\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y # stratify=y ensures both sets have same class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(train_unique, train_counts):\n",
    "    class_name = \"Toxic\" if label == 1 else \"Non-Toxic\"\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  {class_name} ({label}): {count} ({percentage:.1f}%)\")\n",
    "\n",
    "test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
    "for label, count in zip(test_unique, test_counts):\n",
    "    class_name = \"Toxic\" if label == 1 else \"Non-Toxic\"\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  {class_name} ({label}): {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes trained\n",
      "Model saved to ../without_chat_time/model_outputs/naive_bayes_model.joblib\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train, y_train)\n",
    "print(\"Multinomial Naive Bayes trained\")\n",
    "\n",
    "dump(nb_model, f\"{MODELS_DIR}/naive_bayes_model.joblib\")\n",
    "print(f\"Model saved to {MODELS_DIR}/naive_bayes_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../without_chat_time/model_outputs/logistic_regression_model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    solver='lbfgs', # using lbfgs sovler for small data\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "dump(lr_model, f\"{MODELS_DIR}/logistic_regression_model.joblib\")\n",
    "print(f\"Model saved to {MODELS_DIR}/logistic_regression_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Train Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model saved to ../without_chat_time/model_outputs/lda_model.joblib\n",
      "Count vectorizer saved to ../without_chat_time/model_outputs/lda_count_vectorizer.joblib\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.read_csv(f\"{PREPROCESSING_DIR}/conda_cleaned_binary_without_chat_time.csv\")\n",
    "texts = df_cleaned['utterance_cleaned'].fillna('').astype(str).tolist()\n",
    "\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.98\n",
    ")\n",
    "X_count_train = count_vectorizer.fit_transform(texts_train)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lda_model.fit(X_count_train)\n",
    "\n",
    "dump(lda_model, f\"{MODELS_DIR}/lda_model.joblib\")\n",
    "dump(count_vectorizer, f\"{MODELS_DIR}/lda_count_vectorizer.joblib\")\n",
    "print(f\"LDA model saved to {MODELS_DIR}/lda_model.joblib\")\n",
    "print(f\"Count vectorizer saved to {MODELS_DIR}/lda_count_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Topic Distributions + LDA-based Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words per topic:\n",
      "Topic 1: ggwp xd good hahaha game like report plz idiot guy\n",
      "Topic 2: haha fuck game ok go get ur bad play come\n",
      "Topic 3: gg ez mid kill shit time rofl lmao yes w8\n",
      "Topic 4: nice end ty noob wait im wtf commend fucking yeah\n",
      "Topic 5: lol wp report pls sf team sad please pudge dont\n",
      "\n",
      "LDA-based classifier saved to ../without_chat_time/model_outputs/lda_lr_classifier.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/opt/anaconda3/envs/ml_project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n"
     ]
    }
   ],
   "source": [
    "# Display top words for each topic\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "print(f\"\\nTop {n_top_words} words per topic:\")\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")\n",
    "\n",
    "\"\"\"\n",
    "Get topic distributions for training classifier:\n",
    "We need to do this because LDA is unsupervised; so use its topic distributions \n",
    "as features and train a Logistic Regression classifier so we can compare w/ \n",
    "supervised models \n",
    "\"\"\"\n",
    "X_count_test = count_vectorizer.transform(texts_test)\n",
    "doc_topic_dist_train = lda_model.transform(X_count_train)\n",
    "\n",
    "# Create an \"LDA-based classifier\" that can be compared w/ other models\n",
    "lda_lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lda_lr_model.fit(doc_topic_dist_train, y_train)\n",
    "\n",
    "dump(lda_lr_model, f\"{MODELS_DIR}/lda_lr_classifier.joblib\")\n",
    "print(f\"\\nLDA-based classifier saved to {MODELS_DIR}/lda_lr_classifier.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
